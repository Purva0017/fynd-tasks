{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaoWVVlQTvRb",
        "outputId": "840fa555-2f89-4f96-be7a-344a93f12f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'D:\\\\fynd-task-1\\\\.venv\\\\Lib\\\\site-packages\\\\groq\\\\types\\\\file_list_response.py'\n",
            "Check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip -q install kaggle groq pandas numpy tqdm tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdK3o_91XnPH",
        "outputId": "e8e60d47-a0f0-4dc5-ecd6-edd3519d6db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kaggle.json created at: C:\\Users\\Purva A Patel\\.kaggle\\kaggle.json\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "username = \"purva0017\"\n",
        "key = \"KGAT_1037fdb2515cfa882c88005616c2f26e\" # enter your kaggle key here\n",
        "\n",
        "kaggle_dict = {\"username\": username, \"key\": key}\n",
        "\n",
        "# Windows Kaggle location: C:\\Users\\<user>\\.kaggle\\kaggle.json\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "kaggle_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_path, \"w\") as f:\n",
        "    json.dump(kaggle_dict, f)\n",
        "\n",
        "# Kaggle requires permissions like 600 on Linux/macOS.\n",
        "# On Windows chmod doesn't behave the same, but it's fine to keep:\n",
        "try:\n",
        "    os.chmod(kaggle_path, 0o600)\n",
        "except Exception as e:\n",
        "    print(\"chmod skipped on Windows:\", e)\n",
        "\n",
        "print(f\"kaggle.json created at: {kaggle_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpC58y-HYkcc",
        "outputId": "6a4a56f9-423e-4e39-cee3-dddad79c040e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                       title                                                   size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------  ------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "yelp-dataset/yelp-dataset                                 Yelp Dataset                                      4374983563  2022-03-17 22:59:01.257000         153455       1765  0.75             \n",
            "ilhamfp31/yelp-review-dataset                             Yelp Review Sentiment Dataset                      169591198  2020-01-29 06:22:07.300000           4220         36  0.7058824        \n",
            "farukalam/yelp-restaurant-reviews                         Yelp Restaurant Reviews                              3781843  2022-12-31 12:03:48.813000           1944          9  1                \n",
            "omkarsabnis/yelp-reviews-dataset                          Yelp Reviews Dataset                                 3656661  2018-06-03 04:45:50.890000          10370         48  0.29411766       \n",
            "sripaadsrinivasan/yelp-coffee-reviews                     Yelp Coffee Reviews                                  3353888  2021-02-03 12:36:37.573000           1871         14  0.9705882        \n",
            "yacharki/yelp-reviews-for-sa-finegrained-5-classes-csv    üè™ Yelp Reviews for SA fine-grained 5 classes CSV   199985576  2024-09-03 15:17:28.953000            723         63  1                \n",
            "joebeachcapital/mcdonalds-yelp-reviews                    McDonalds Yelp! Reviews                               333478  2023-09-04 00:11:50.597000            576         14  1                \n",
            "yacharki/yelp-reviews-for-sentianalysis-binary-np-csv     üè™Yelp Reviews for Senti-Analysis Binary -N/P+      169583717  2022-05-09 09:34:30.803000            618         23  1                \n",
            "fireballbyedimyrnmom/yelp-dataset                         Yelp dataset                                      3899332354  2020-02-14 19:30:25.437000            667         12  0.4375           \n",
            "vivekhn/yelp-reviews                                      Yelp Reviews                                         3656661  2020-07-18 19:16:11.067000            353          8  0.5882353        \n",
            "sahilnbajaj/yelp-dataset                                  yelp_dataset                                               0  2024-04-09 15:18:49.603000             64          9  1                \n",
            "brkurzawa/us-breweries                                    US Breweries                                           77501  2019-10-02 03:15:27.660000           5001        115  1                \n",
            "yacharki/binary-classification-90-yelp-reviews-cnn        ü¶æ Pre-trained model Binary CNN NLP Yelp Reviews    494690391  2022-05-09 15:06:50.320000             65         22  1                \n",
            "kanchana1990/usa-yelp-eats-2024-top-dining-gems-revealed  üç¥USA Yelp Eats 2024: Top Dining Gems Revealed üåü        46473  2024-03-07 07:56:25.173000            239         45  1                \n",
            "z5025122/yelp-csv                                         Yelp CSV                                           370165268  2018-10-18 00:45:36.950000           1382         10  0.1764706        \n",
            "thedevastator/yelp-reviews-sentiment-dataset              Yelp Reviews Sentiment Dataset                     169587518  2022-11-25 10:06:15.660000            847          8  1                \n",
            "channingfisher/yelp-text-sentiment-analysis-2015          Yelp Text Sentiment Analysis 2015                      82239  2024-02-16 12:21:35.573000             98          8  0.9375           \n",
            "irustandi/yelp-review-polarity                            Yelp Review Polarity                               339182524  2019-04-19 02:08:10.147000           2087          7  0.1764706        \n",
            "devmaxime/yelp-photos                                     Yelp photos                                       6117847846  2021-04-14 11:46:33.350000            112          4  0.375            \n",
            "jcraggy/asian-restaurants                                 Asian Restaurants                                       3435  2021-06-20 21:21:09.067000            594         11  1                \n"
          ]
        }
      ],
      "source": [
        "!pip -q install kaggle\n",
        "!kaggle datasets list -s yelp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkqj-YrwYxlo",
        "outputId": "5a25139e-afdf-46e1-cb64-c54160fce79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\n",
            "License(s): other\n",
            "Downloading yelp-reviews-dataset.zip to d:\\fynd-task-1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/3.49M [00:00<?, ?B/s]\n",
            " 29%|‚ñà‚ñà‚ñä       | 1.00M/3.49M [00:01<00:03, 680kB/s]\n",
            " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2.00M/3.49M [00:01<00:01, 1.37MB/s]\n",
            " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 3.00M/3.49M [00:02<00:00, 1.77MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.49M/3.49M [00:02<00:00, 1.63MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yelp.csv\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d omkarsabnis/yelp-reviews-dataset\n",
        "!unzip -q yelp-reviews-dataset.zip -d yelp_data\n",
        "!ls yelp_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "73ZIBwrrY9GW",
        "outputId": "895837d1-88f2-471f-9966-330e4a0a3f51"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We got here around midnight last Friday... the...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My husband and I were really, really disappoin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  stars\n",
              "0  We got here around midnight last Friday... the...      4\n",
              "1  Brought a friend from Louisiana here.  She say...      5\n",
              "2  Every friday, my dad and I eat here. We order ...      3\n",
              "3  My husband and I were really, really disappoin...      1\n",
              "4  Love this place!  Was in phoenix 3 weeks for w...      5"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Adjust filename based on what appears in yelp_data folder\n",
        "df = pd.read_csv(\"yelp_data/yelp.csv\")  # change if file name differs\n",
        "df = df.dropna(subset=[\"text\", \"stars\"])  # ensure clean\n",
        "\n",
        "# sample for evaluation\n",
        "sample_df = df.sample(n=200, random_state=42).reset_index(drop=True)\n",
        "sample_df = sample_df[[\"text\", \"stars\"]]\n",
        "sample_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf0613TOZP0N",
        "outputId": "bb92a379-2d12-492e-f6c4-1177944ca28b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from groq import Groq\n",
        "\n",
        "# 4 keys: one for each prompt run + one for consistency\n",
        "os.environ[\"GROQ_API_KEY_V1\"] = getpass(\"Enter Groq API Key for V1: \")\n",
        "os.environ[\"GROQ_API_KEY_V2\"] = getpass(\"Enter Groq API Key for V2: \")\n",
        "os.environ[\"GROQ_API_KEY_V3\"] = getpass(\"Enter Groq API Key for V3: \")\n",
        "os.environ[\"GROQ_API_KEY_CONSISTENCY\"] = getpass(\"Enter Groq API Key for CONSISTENCY: \")\n",
        "\n",
        "client_v1 = Groq(api_key=os.environ[\"GROQ_API_KEY_V1\"])\n",
        "client_v2 = Groq(api_key=os.environ[\"GROQ_API_KEY_V2\"])\n",
        "client_v3 = Groq(api_key=os.environ[\"GROQ_API_KEY_V3\"])\n",
        "client_cons = Groq(api_key=os.environ[\"GROQ_API_KEY_CONSISTENCY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "epUqGvvfZxPS"
      },
      "outputs": [],
      "source": [
        "def prompt_v1(review):\n",
        "    return f\"\"\"\n",
        "You are a Yelp review star rating classifier.\n",
        "\n",
        "Decide an integer rating from 1 to 5 based on the review sentiment.\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Output ONLY a valid JSON object.\n",
        "- Do NOT output markdown, backticks, code fences, headings, or any extra text.\n",
        "- Use EXACTLY these keys: \"predicted_stars\", \"explanation\"\n",
        "- \"predicted_stars\" must be an integer in [1,2,3,4,5].\n",
        "- \"explanation\" must be <= 15 words (one sentence).\n",
        "- If unsure, choose the closest rating. Never output null.\n",
        "\n",
        "Rating guide:\n",
        "1 = very negative, terrible experience\n",
        "2 = mostly negative\n",
        "3 = mixed/neutral\n",
        "4 = mostly positive with minor issues\n",
        "5 = extremely positive, excellent\n",
        "\n",
        "Review:\n",
        "{review}\n",
        "\n",
        "Return ONLY JSON:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def prompt_v2(review):\n",
        "    return f\"\"\"\n",
        "You are an expert Yelp review rating model.\n",
        "\n",
        "Your job: predict the Yelp star rating (1-5) that best matches the reviewer‚Äôs overall satisfaction.\n",
        "\n",
        "Decision factors (in priority order):\n",
        "1) Overall satisfaction (recommend or not)\n",
        "2) Severity of complaints (major vs minor)\n",
        "3) Service quality\n",
        "4) Product/food quality\n",
        "5) Price/value\n",
        "\n",
        "Rating rubric:\n",
        "5 = Excellent: strongly recommended, no meaningful complaints\n",
        "4 = Good: mostly positive, minor complaint(s)\n",
        "3 = Average: mixed feelings or neutral experience\n",
        "2 = Poor: mostly negative, multiple problems, not recommended\n",
        "1 = Terrible: extremely negative, unacceptable experience\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Output ONLY a valid JSON object.\n",
        "- No markdown, no backticks, no extra words.\n",
        "- JSON must match this schema exactly:\n",
        "  {{\n",
        "    \"predicted_stars\": <integer 1-5>,\n",
        "    \"explanation\": \"<one sentence, <= 15 words>\"\n",
        "  }}\n",
        "- If uncertain between two ratings, pick the lower one.\n",
        "- Never output null.\n",
        "\n",
        "Review:\n",
        "{review}\n",
        "\n",
        "Return ONLY JSON:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def prompt_v3(review):\n",
        "    return f\"\"\"\n",
        "You are a Yelp review star rating classifier.\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Output ONLY a valid JSON object.\n",
        "- Do NOT output markdown/backticks/extra text.\n",
        "- Use EXACTLY these keys: \"predicted_stars\", \"explanation\"\n",
        "- \"predicted_stars\" must be integer 1-5.\n",
        "- \"explanation\" must be <= 15 words.\n",
        "- If unsure, choose the closest rating (prefer lower if borderline).\n",
        "\n",
        "Rubric:\n",
        "5 = Excellent, very satisfied, would strongly recommend\n",
        "4 = Good, satisfied, minor issues\n",
        "3 = Mixed/average, neutral or balanced positives and negatives\n",
        "2 = Negative, dissatisfied, several problems\n",
        "1 = Very negative, unacceptable, angry tone\n",
        "\n",
        "Examples:\n",
        "Review: \"Absolutely amazing! Delicious food, friendly staff, and fast service.\"\n",
        "Output: {{\"predicted_stars\": 5, \"explanation\": \"Strongly positive experience with no meaningful complaints.\"}}\n",
        "\n",
        "Review: \"Good food but service was slow and the place was noisy.\"\n",
        "Output: {{\"predicted_stars\": 4, \"explanation\": \"Mostly positive experience with a minor service issue.\"}}\n",
        "\n",
        "Review: \"It was okay. Nothing special. Might try again but not excited.\"\n",
        "Output: {{\"predicted_stars\": 3, \"explanation\": \"Neutral review with no strong positives or negatives.\"}}\n",
        "\n",
        "Review: \"Bad service and overpriced. Food arrived late and was bland.\"\n",
        "Output: {{\"predicted_stars\": 2, \"explanation\": \"Mostly negative experience with multiple issues.\"}}\n",
        "\n",
        "Review: \"Worst experience ever. Staff was rude and food was inedible.\"\n",
        "Output: {{\"predicted_stars\": 1, \"explanation\": \"Extremely negative experience with unacceptable service and quality.\"}}\n",
        "\n",
        "Now classify this review:\n",
        "\n",
        "Review:\n",
        "{review}\n",
        "\n",
        "Return ONLY JSON:\n",
        "\"\"\".strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-0GzFlW7Z7LK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json(text):\n",
        "    \"\"\"\n",
        "    Extract JSON object from a model response safely.\n",
        "    Returns dict or None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # fallback: find first {...} block\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group())\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def validate_output(data):\n",
        "    \"\"\"\n",
        "    Validate JSON structure and constraints.\n",
        "    \"\"\"\n",
        "    if not isinstance(data, dict):\n",
        "        return False\n",
        "\n",
        "    if \"predicted_stars\" not in data or \"explanation\" not in data:\n",
        "        return False\n",
        "\n",
        "    if not isinstance(data[\"predicted_stars\"], int):\n",
        "        return False\n",
        "\n",
        "    if data[\"predicted_stars\"] < 1 or data[\"predicted_stars\"] > 5:\n",
        "        return False\n",
        "\n",
        "    if not isinstance(data[\"explanation\"], str) or len(data[\"explanation\"].strip()) == 0:\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1_wwrZ0waG6p"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "def groq_predict(prompt, client):\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=80\n",
        "    )\n",
        "    return resp.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yp6rR6kBaRSQ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def run_prompt_eval(sample_df, prompt_fn, name, client, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Runs prompt evaluation with checkpointing.\n",
        "    If checkpoint exists, resumes from last completed review_id.\n",
        "    \"\"\"\n",
        "\n",
        "    # Resume if file exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        existing = pd.read_csv(checkpoint_path)\n",
        "        done_ids = set(existing[\"review_id\"].astype(int).tolist())\n",
        "        print(f\"[{name}] Resuming from checkpoint. Already done: {len(done_ids)} rows.\")\n",
        "    else:\n",
        "        existing = None\n",
        "        done_ids = set()\n",
        "        print(f\"[{name}] No checkpoint found. Starting fresh.\")\n",
        "\n",
        "    rows_to_run = [i for i in range(len(sample_df)) if i not in done_ids]\n",
        "\n",
        "    out_rows = []  # buffer\n",
        "\n",
        "    for i in tqdm(rows_to_run, desc=f\"Running {name}\"):\n",
        "        review = sample_df.loc[i, \"text\"]\n",
        "        actual = int(sample_df.loc[i, \"stars\"])\n",
        "\n",
        "        raw = groq_predict(prompt_fn(review), client=client)\n",
        "        parsed = extract_json(raw)\n",
        "\n",
        "        valid = validate_output(parsed)\n",
        "        predicted = parsed[\"predicted_stars\"] if valid else None\n",
        "        explanation = parsed[\"explanation\"] if valid else None\n",
        "\n",
        "        out_rows.append({\n",
        "            \"review_id\": i,\n",
        "            \"actual_stars\": actual,\n",
        "            \"predicted_stars\": predicted,\n",
        "            \"valid_json\": valid,\n",
        "            \"explanation\": explanation,\n",
        "            \"raw_output\": raw\n",
        "        })\n",
        "\n",
        "        # Append every row immediately (no data loss)\n",
        "        pd.DataFrame(out_rows).to_csv(\n",
        "            checkpoint_path,\n",
        "            mode=\"a\",\n",
        "            header=not os.path.exists(checkpoint_path),\n",
        "            index=False\n",
        "        )\n",
        "        out_rows = []\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    # Load full results\n",
        "    return pd.read_csv(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "pNIrxAQ9aVc-",
        "outputId": "57d638e7-29c5-4375-9d83-2a6259374947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt V1] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Prompt V1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [10:32<00:00,  3.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt V2] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Prompt V2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [07:40<00:00,  2.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt V3] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Prompt V3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [07:22<01:18,  2.61s/it]\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kf94yq8pe8stpatzht7dy0cc` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99972, Requested 479. Please try again in 6m29.664s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m res_v1 = run_prompt_eval(sample_df, prompt_v1, \u001b[33m\"\u001b[39m\u001b[33mPrompt V1\u001b[39m\u001b[33m\"\u001b[39m, client_v1, \u001b[33m\"\u001b[39m\u001b[33mcheckpoint_v1.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m res_v2 = run_prompt_eval(sample_df, prompt_v2, \u001b[33m\"\u001b[39m\u001b[33mPrompt V2\u001b[39m\u001b[33m\"\u001b[39m, client_v2, \u001b[33m\"\u001b[39m\u001b[33mcheckpoint_v2.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m res_v3 = \u001b[43mrun_prompt_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_v3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPrompt V3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_v3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoint_v3.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrun_prompt_eval\u001b[39m\u001b[34m(sample_df, prompt_fn, name, client, checkpoint_path)\u001b[39m\n\u001b[32m     27\u001b[39m review = sample_df.loc[i, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m actual = \u001b[38;5;28mint\u001b[39m(sample_df.loc[i, \u001b[33m\"\u001b[39m\u001b[33mstars\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m raw = \u001b[43mgroq_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m parsed = extract_json(raw)\n\u001b[32m     33\u001b[39m valid = validate_output(parsed)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mgroq_predict\u001b[39m\u001b[34m(prompt, client)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgroq_predict\u001b[39m(prompt, client):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp.choices[\u001b[32m0\u001b[39m].message.content\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\fynd-task-1\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\fynd-task-1\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\fynd-task-1\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kf94yq8pe8stpatzht7dy0cc` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99972, Requested 479. Please try again in 6m29.664s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "res_v1 = run_prompt_eval(sample_df, prompt_v1, \"Prompt V1\", client_v1, \"checkpoint_v1.csv\")\n",
        "res_v2 = run_prompt_eval(sample_df, prompt_v2, \"Prompt V2\", client_v2, \"checkpoint_v2.csv\")\n",
        "res_v3 = run_prompt_eval(sample_df, prompt_v3, \"Prompt V3\", client_v3, \"checkpoint_v3.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# New key ONLY for continuing V3\n",
        "os.environ[\"GROQ_API_KEY_V3B\"] = getpass(\"Enter NEW Groq API Key for V3 continuation: \")\n",
        "client_v3b = Groq(api_key=os.environ[\"GROQ_API_KEY_V3B\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Prompt V3] Resuming from checkpoint. Already done: 171 rows.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Prompt V3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:24<00:00,  1.17it/s]\n"
          ]
        }
      ],
      "source": [
        "res_v3 = run_prompt_eval(sample_df, prompt_v3, \"Prompt V3\", client_v3b, \"checkpoint_v3.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved combined CSV: all_prompt_outputs.csv ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "# Rename columns so they don't clash\n",
        "res_v1_ren = res_v1.rename(columns={\n",
        "    \"predicted_stars\": \"v1_predicted_stars\",\n",
        "    \"valid_json\": \"v1_valid_json\",\n",
        "    \"explanation\": \"v1_explanation\",\n",
        "    \"raw_output\": \"v1_raw_output\"\n",
        "})\n",
        "\n",
        "res_v2_ren = res_v2.rename(columns={\n",
        "    \"predicted_stars\": \"v2_predicted_stars\",\n",
        "    \"valid_json\": \"v2_valid_json\",\n",
        "    \"explanation\": \"v2_explanation\",\n",
        "    \"raw_output\": \"v2_raw_output\"\n",
        "})\n",
        "\n",
        "res_v3_ren = res_v3.rename(columns={\n",
        "    \"predicted_stars\": \"v3_predicted_stars\",\n",
        "    \"valid_json\": \"v3_valid_json\",\n",
        "    \"explanation\": \"v3_explanation\",\n",
        "    \"raw_output\": \"v3_raw_output\"\n",
        "})\n",
        "\n",
        "# Merge on review_id + actual_stars\n",
        "merged = res_v1_ren.merge(\n",
        "    res_v2_ren[[\"review_id\", \"v2_predicted_stars\", \"v2_valid_json\", \"v2_explanation\", \"v2_raw_output\"]],\n",
        "    on=\"review_id\",\n",
        "    how=\"outer\"\n",
        ").merge(\n",
        "    res_v3_ren[[\"review_id\", \"v3_predicted_stars\", \"v3_valid_json\", \"v3_explanation\", \"v3_raw_output\"]],\n",
        "    on=\"review_id\",\n",
        "    how=\"outer\"\n",
        ")\n",
        "\n",
        "# Ensure actual_stars exists\n",
        "if \"actual_stars\" not in merged.columns:\n",
        "    merged[\"actual_stars\"] = sample_df[\"stars\"].astype(int)\n",
        "\n",
        "# Save combined\n",
        "merged.to_csv(\"all_prompt_outputs.csv\", index=False)\n",
        "print(\"Saved combined CSV: all_prompt_outputs.csv ‚úÖ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8HBOKOcwhAPe"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(df):\n",
        "    json_validity = df[\"valid_json\"].mean() * 100\n",
        "    accuracy = (df[\"actual_stars\"] == df[\"predicted_stars\"]).mean() * 100\n",
        "    return accuracy, json_validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl5ywyoDhD-t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def consistency_test_checkpointed(sample_df, prompt_fn, client, n=20, repeats=3, checkpoint_path=\"consistency_checkpoint.csv\"):\n",
        "    \"\"\"\n",
        "    Checkpointed consistency test:\n",
        "    - Saves each (repeat, review_id) prediction immediately to CSV\n",
        "    - Resumes without recomputing already-done repeat rows\n",
        "    - Returns consistency percentage\n",
        "    \"\"\"\n",
        "\n",
        "    subset = sample_df.sample(n=n, random_state=7).reset_index(drop=True)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        existing = pd.read_csv(checkpoint_path)\n",
        "        done = set(zip(existing[\"repeat\"].astype(int), existing[\"review_id\"].astype(int)))\n",
        "        print(f\"[Consistency] Resuming. Done predictions: {len(done)}\")\n",
        "    else:\n",
        "        existing = None\n",
        "        done = set()\n",
        "        print(\"[Consistency] No checkpoint found. Starting fresh.\")\n",
        "\n",
        "    # Run and checkpoint each prediction\n",
        "    for r in range(repeats):\n",
        "        for i in tqdm(range(len(subset)), desc=f\"Consistency run {r+1}/{repeats}\"):\n",
        "            if (r, i) in done:\n",
        "                continue\n",
        "\n",
        "            raw = groq_predict(prompt_fn(subset.loc[i, \"text\"]), client=client)\n",
        "            parsed = extract_json(raw)\n",
        "\n",
        "            if validate_output(parsed):\n",
        "                pred = parsed[\"predicted_stars\"]\n",
        "                valid = True\n",
        "            else:\n",
        "                pred = None\n",
        "                valid = False\n",
        "\n",
        "            # Save immediately (append mode)\n",
        "            pd.DataFrame([{\n",
        "                \"repeat\": r,\n",
        "                \"review_id\": i,\n",
        "                \"actual_stars\": int(subset.loc[i, \"stars\"]),\n",
        "                \"predicted_stars\": pred,\n",
        "                \"valid_json\": valid,\n",
        "                \"raw_output\": raw\n",
        "            }]).to_csv(\n",
        "                checkpoint_path,\n",
        "                mode=\"a\",\n",
        "                header=not os.path.exists(checkpoint_path),\n",
        "                index=False\n",
        "            )\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "    # Load all and compute consistency\n",
        "    dfc = pd.read_csv(checkpoint_path)\n",
        "\n",
        "    # Pivot into [review_id x repeats]\n",
        "    pivot = dfc.pivot_table(index=\"review_id\", columns=\"repeat\", values=\"predicted_stars\", aggfunc=\"first\")\n",
        "\n",
        "    consistent = 0\n",
        "    valid_total = 0\n",
        "\n",
        "    for i in pivot.index:\n",
        "        row = pivot.loc[i].tolist()\n",
        "        # must have all repeats present and non-null\n",
        "        if any(pd.isna(x) for x in row):\n",
        "            continue\n",
        "        valid_total += 1\n",
        "        if len(set(row)) == 1:\n",
        "            consistent += 1\n",
        "\n",
        "    if valid_total == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (consistent / valid_total) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "mi2SBZbRjDtL",
        "outputId": "1d46dcfd-ece1-40ce-d48c-6b9301eab18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Consistency] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Consistency run 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:10<00:00,  1.98it/s]\n",
            "Consistency run 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:29<00:00,  1.49s/it]\n",
            "Consistency run 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:51<00:00,  2.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Consistency] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Consistency run 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:52<00:00,  2.61s/it]\n",
            "Consistency run 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:51<00:00,  2.55s/it]\n",
            "Consistency run 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:52<00:00,  2.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Consistency] No checkpoint found. Starting fresh.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Consistency run 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:52<00:00,  2.62s/it]\n",
            "Consistency run 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:50<00:00,  2.54s/it]\n",
            "Consistency run 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:51<00:00,  2.59s/it]\n"
          ]
        }
      ],
      "source": [
        "cons_v1 = consistency_test_checkpointed(sample_df, prompt_v1, client=client_cons,\n",
        "                                       checkpoint_path=\"consistency_v1.csv\")\n",
        "\n",
        "cons_v2 = consistency_test_checkpointed(sample_df, prompt_v2, client=client_cons,\n",
        "                                       checkpoint_path=\"consistency_v2.csv\")\n",
        "\n",
        "cons_v3 = consistency_test_checkpointed(sample_df, prompt_v3, client=client_cons,\n",
        "                                       checkpoint_path=\"consistency_v3.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "giBLi4DFjIh4",
        "outputId": "36d6903d-0fcd-4695-d03a-247937aeddcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Approach                                    |   Accuracy (%) |   JSON Validity (%) |   Consistency (%) |\n",
            "|---------------------------------------------|----------------|---------------------|-------------------|\n",
            "| Prompt V1 (Strict JSON + Short Rubric)      |           56.5 |                 100 |               100 |\n",
            "| Prompt V2 (Detailed Rubric + Locked Rules)  |           63   |                 100 |               100 |\n",
            "| Prompt V3 (Few-shot + Rubric + Strict JSON) |           58   |                 100 |               100 |\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "acc1, val1 = compute_metrics(res_v1)\n",
        "acc2, val2 = compute_metrics(res_v2)\n",
        "acc3, val3 = compute_metrics(res_v3)\n",
        "\n",
        "comparison = pd.DataFrame([\n",
        "    [\"Prompt V1 (Strict JSON + Short Rubric)\", acc1, val1, cons_v1],\n",
        "    [\"Prompt V2 (Detailed Rubric + Locked Rules)\", acc2, val2, cons_v2],\n",
        "    [\"Prompt V3 (Few-shot + Rubric + Strict JSON)\", acc3, val3, cons_v3]\n",
        "], columns=[\"Approach\", \"Accuracy (%)\", \"JSON Validity (%)\", \"Consistency (%)\"])\n",
        "\n",
        "print(tabulate(comparison, headers=\"keys\", tablefmt=\"github\", showindex=False))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
